{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/sinaziaee/mini_conda/miniconda3/envs/test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nibabel as nib\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import utils as utils\n",
    "from scipy.ndimage import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_path(path=None):\n",
    "    if path:\n",
    "        return path\n",
    "    else:\n",
    "        return os.getcwd()\n",
    "\n",
    "new_folder = 'data_npy2'\n",
    "\n",
    "if not os.path.exists(f'{get_current_path()}/{new_folder}'):\n",
    "    os.mkdir(f'{get_current_path()}/{new_folder}')\n",
    "    \n",
    "if not os.path.exists(f'{get_current_path()}/{new_folder}/train'):\n",
    "    os.mkdir(f'{get_current_path()}/{new_folder}/train')\n",
    "    \n",
    "if not os.path.exists(f'{get_current_path()}/{new_folder}/valid'):\n",
    "    os.mkdir(f'{get_current_path()}/{new_folder}/valid')\n",
    "    \n",
    "if not os.path.exists(f'{get_current_path()}/{new_folder}/test'):\n",
    "    os.mkdir(f'{get_current_path()}/{new_folder}/test')\n",
    "    \n",
    "def make_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "train_image_path = make_path(f'{get_current_path()}/{new_folder}/train/image')\n",
    "train_segment_path = make_path(f'{get_current_path()}/{new_folder}/train/segmentation')\n",
    "valid_image_path = make_path(f'{get_current_path()}/{new_folder}/valid/image')\n",
    "valid_segment_path = make_path(f'{get_current_path()}/{new_folder}/valid/segmentation')\n",
    "test_image_path = make_path(f'{get_current_path()}/{new_folder}/test/image')\n",
    "test_segment_path = make_path(f'{get_current_path()}/{new_folder}/test/segmentation')\n",
    "\n",
    "src_train_image_path = '3d_original_padded_images/training/images'\n",
    "src_valid_image_path = '3d_original_padded_images/validation/images'\n",
    "src_test_image_path = '3d_original_padded_images/testing/images' \n",
    "\n",
    "src_train_mask_path = '3d_original_padded_images/training/labels'\n",
    "src_valid_mask_path = '3d_original_padded_images/validation/labels'\n",
    "src_test_mask_path = '3d_original_padded_images/testing/labels' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [13:47<00:00, 14.02s/it]\n",
      " 50%|█████     | 5/10 [01:11<01:11, 14.23s/it]"
     ]
    }
   ],
   "source": [
    "def perform_image_slicing(root_path, saving_path):\n",
    "    count = 0\n",
    "    depth_list = []\n",
    "    for file_name in tqdm(sorted(os.listdir(root_path))):\n",
    "        # print(os.path.join(root_path, file_name))\n",
    "        if file_name.endswith('.nii.gz'):\n",
    "            img_id = str(file_name.split('_')[1])\n",
    "            img = nib.load(os.path.join(root_path, file_name)).get_fdata()\n",
    "            # normalizing the image between 0 and 1\n",
    "            img=(img-img.min())/(max((img.max()-img.min()),1e-3))\n",
    "            depth = img.shape[2]\n",
    "            depth_list.append(depth)\n",
    "            for j in range(depth):\n",
    "                new_path=os.path.join(saving_path, '{:05d}.npy'.format(j+count))\n",
    "                new_img = torch.tensor(img[:, :, j:j+1].astype(np.float32))\n",
    "                new_img = new_img.permute(2, 0, 1)\n",
    "                new_img = np.array(new_img)\n",
    "                new_img = np.squeeze(new_img, axis=0)\n",
    "                new_img = rotate(new_img, 90)\n",
    "                new_img = np.expand_dims(new_img, axis=0)\n",
    "                np.save(new_path, new_img)\n",
    "            count += depth\n",
    "    return depth_list\n",
    "\n",
    "train_depth_list = perform_image_slicing(root_path=src_train_image_path, saving_path=train_image_path)\n",
    "valid_depth_list = perform_image_slicing(root_path=src_valid_image_path, saving_path=valid_image_path)\n",
    "test_depth_list = perform_image_slicing(root_path=src_test_image_path, saving_path=test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "depth_lists = {\n",
    "    'train': train_depth_list,\n",
    "    'valid': valid_depth_list,\n",
    "    'test': test_depth_list\n",
    "}\n",
    "\n",
    "json_file_path = f'{new_folder}/depth_lists.json'\n",
    "\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(depth_lists, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[240, 195, 216, 221, 223, 201, 205, 196, 206, 221, 214, 186, 231, 310, 223, 210, 218, 223, 310, 310, 310, 205, 310, 233, 310, 205, 227, 223, 198, 201, 251, 237, 310, 216, 198, 201, 206, 222, 310, 209, 218, 193, 215, 228, 310, 203, 223, 211, 213, 216, 209, 194, 310, 201, 224, 210, 233, 310, 209]\n",
      "186 310\n",
      "185 466\n",
      "181 310\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = f'{new_folder}/depth_lists.json'\n",
    "\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "print(data['train'])\n",
    "print(min(data['train']), max(data['train']))\n",
    "print(min(data['valid']), max(data['valid']))\n",
    "print(min(data['test']), max(data['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 37/59 [04:34<02:43,  7.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m             np\u001b[38;5;241m.\u001b[39msave(new_path,seg_2ch)\n\u001b[1;32m     24\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m depth\n\u001b[0;32m---> 26\u001b[0m \u001b[43mperform_segment_slicing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_train_mask_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaving_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_segment_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m perform_segment_slicing(root_path\u001b[38;5;241m=\u001b[39msrc_valid_mask_path, saving_path\u001b[38;5;241m=\u001b[39mvalid_segment_path)\n\u001b[1;32m     28\u001b[0m perform_segment_slicing(root_path\u001b[38;5;241m=\u001b[39msrc_test_mask_path, saving_path\u001b[38;5;241m=\u001b[39mtest_segment_path)\n",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36mperform_segment_slicing\u001b[0;34m(root_path, saving_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(root_path))):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# print(os.path.join(root_path, file_name))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     seg_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(file_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     seg \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     seg_no_cancer\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mwhere(seg\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     10\u001b[0m     depth \u001b[38;5;241m=\u001b[39m seg_no_cancer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/mini_conda/miniconda3/envs/test/lib/python3.11/site-packages/nibabel/dataobj_images.py:373\u001b[0m, in \u001b[0;36mDataobjImage.get_fdata\u001b[0;34m(self, caching, dtype)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Always return requested data type\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# during scaling\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataobj, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caching \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/mini_conda/miniconda3/envs/test/lib/python3.11/site-packages/nibabel/arrayproxy.py:439\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    419\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/mini_conda/miniconda3/envs/test/lib/python3.11/site-packages/nibabel/arrayproxy.py:406\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    404\u001b[0m     scl_inter \u001b[38;5;241m=\u001b[39m scl_inter\u001b[38;5;241m.\u001b[39mastype(use_dtype)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_unscaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslicer\u001b[49m\u001b[43m)\u001b[49m, scl_slope, scl_inter)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m scaled\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mpromote_types(scaled\u001b[38;5;241m.\u001b[39mdtype, dtype), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/mini_conda/miniconda3/envs/test/lib/python3.11/site-packages/nibabel/arrayproxy.py:376\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canonical_slicers(slicer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m==\u001b[39m canonical_slicers(\n\u001b[1;32m    373\u001b[0m     (), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    374\u001b[0m ):\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 376\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m            \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fileslice(\n\u001b[1;32m    386\u001b[0m         fileobj,\n\u001b[1;32m    387\u001b[0m         slicer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m         lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock,\n\u001b[1;32m    393\u001b[0m     )\n",
      "File \u001b[0;32m~/mini_conda/miniconda3/envs/test/lib/python3.11/site-packages/nibabel/volumeutils.py:464\u001b[0m, in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    462\u001b[0m infile\u001b[38;5;241m.\u001b[39mseek(offset)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(infile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 464\u001b[0m     data_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mbytearray\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     n_read \u001b[38;5;241m=\u001b[39m infile\u001b[38;5;241m.\u001b[39mreadinto(data_bytes)\n\u001b[1;32m    466\u001b[0m     needs_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def perform_segment_slicing(root_path, saving_path):\n",
    "    count = 0\n",
    "    depth_list = []\n",
    "    for file_name in tqdm(sorted(os.listdir(root_path))):\n",
    "        # print(os.path.join(root_path, file_name))\n",
    "        seg_id = str(file_name.split('_')[1]).split('.')[0]\n",
    "        seg = nib.load(os.path.join(root_path, file_name)).get_fdata()\n",
    "        seg_no_cancer=np.where(seg>0,1,0).astype(np.uint8)\n",
    "        depth = seg_no_cancer.shape[2]\n",
    "        depth_list.append(depth)\n",
    "        for j in range(depth):\n",
    "            new_path=os.path.join(saving_path, '{:05d}.npy'.format(j+count))\n",
    "            seg_1ch=torch.tensor(seg_no_cancer[:, :,j:j+1],dtype=torch.int64)\n",
    "            seg_1ch = np.array(seg_1ch)\n",
    "            seg_1ch = np.squeeze(seg_1ch, axis=2)\n",
    "            seg_1ch = rotate(seg_1ch, 90)\n",
    "            seg_1ch = np.expand_dims(seg_1ch, axis=2)\n",
    "            seg_1ch = torch.tensor(seg_1ch)\n",
    "            seg_2ch=F.one_hot(seg_1ch,num_classes=2)\n",
    "            seg_2ch=torch.squeeze(seg_2ch.permute(3,0,1,2))\n",
    "            seg_2ch=np.array(seg_2ch,dtype=np.uint8)\n",
    "            np.save(new_path,seg_2ch)\n",
    "        count += depth\n",
    "\n",
    "perform_segment_slicing(root_path=src_train_mask_path, saving_path=train_segment_path)\n",
    "perform_segment_slicing(root_path=src_valid_mask_path, saving_path=valid_segment_path)\n",
    "perform_segment_slicing(root_path=src_test_mask_path, saving_path=test_segment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_slices(imaging_path, segmentation_path):\n",
    "    imaging_slice = np.load(imaging_path)\n",
    "    segmentation_slice = np.load(segmentation_path)\n",
    "\n",
    "    # Squeeze the singleton dimension if it exists\n",
    "    imaging_slice = np.transpose(imaging_slice, (1, 2, 0))\n",
    "    imaging_slice = np.squeeze(imaging_slice)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(imaging_slice, cmap='gray')\n",
    "    ax.imshow(segmentation_slice[1], cmap='Reds', alpha=0.3)\n",
    "    ax.set_title('Imaging Slice with Segmentation')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(imaging_slice, cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
